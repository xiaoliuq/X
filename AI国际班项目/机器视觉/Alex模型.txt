AlexNet的特点：1.更深的网络结构 2.使用层叠的卷积层 即卷积层+卷积层+池化层来提取图像的特征 3.使用Dropout抑制过拟合4.使用Relu替换之前的sigmoid的作为激活函数
网络包含8个带权重的层；前5层是卷积层，剩下的3层是全连接层。
在最初，sigmoid和tanh函数最常用的激活函数
sigmoid作为激活函数：在网络层数较少时，sigmoid函数的特性能够很好的满足激活函数的作用：它把一个实数压缩至0到1之间，当输入的数字非常大的时候，结果会接近1；当输入非常大的负数时，则会得到接近0的结果。这种特性，能够很好的模拟神经元在受刺激后，是否被激活向后传递信息（输出为0，几乎不被激活；输出为1，完全被激活）。
sigmoid一个很大的问题就是梯度饱和。 
ReLu作为激活函数：针对sigmoid梯度饱和导致训练收敛慢的问题，在AlexNet中引入了ReLU。ReLU是一个分段线性函数，小于等于0则输出为0；大于0的则恒等输出。